{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbus Ship Detection\n",
    "\n",
    "EE 435: Deep Learning Foundations from Scratch \\\n",
    "Northwestern Univertiy Winter 2020\n",
    "\n",
    "## Memebers\n",
    "* Blaine Rothrock (`BRY4768`)\n",
    "* Ilan Ponsky (`IPW1530`)\n",
    "* Will Dong (`WDG4518`)\n",
    "\n",
    "## Code\n",
    "\n",
    "This document is intenially missing components of our implementation and is intended as a over view what we did and learned from the project. For all notebooks and code, see our public [git repo](https://github.com/blainerothrock/airbus-ship-detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Our group was interested in applying knowledge from this course to training TensorFlow models and getting a better understanding of neural networks involved in image processing. In order to learn about this process in an organized and efficient way, we utilized a closed Kaggle competition that centered around our topic of image processing with neural networks. The competition we used was the [Airbus Ship Detection Challenge](https://www.kaggle.com/c/airbus-ship-detection). The goal of the Kaggle competition was for participants to be able to build a model that could “detect all ships in satellite images as quickly as possible.”\n",
    "\n",
    "The specific goals we wanted to hit for our Deep Learning from Scratch final project were to: \n",
    "* Build a binary classifier model to gain a basic understanding of Tensorflow and how to build models in TensorFlow.\n",
    "    - The objective of the binary classifier model is to output whether an image contained a ship or not utilizing optimazation techniquies in TensorFlow.\n",
    "* Explore and implement a U-net model for image segmentation. \n",
    "    - U-Net models are the current state-of-art for image segmentation and where most started for this competition.\n",
    "    - This is a heafty goal given the data size of ~40GB of images and the time it takes to training this complex model. Our goal is to build a model, attempt at training, and gain a understanding of the U-Net architechure.\n",
    "    - To accomplish this, we utilized the notebook of Kevin Mader on Kaggle which served as an excellent foundation to get started with implementing the mentioned goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data\n",
    "\n",
    "The main reason we choose to use Kaggle is because the data is easily obtained and well organized. The data came in the form of a set of training and testing images then a `.csv` file containing image names and encoded pixels. Each row mapped to a image and the encoded pixel column represetned a bounding box around a ship in a image. Most images have no ships and some have multiple, up to 15.\n",
    "\n",
    "First we are able to explore the segment file: `train_ship_segmentations_v2.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231723 masks found\n",
      "192556 unique images found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00003e153.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001124c7.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000155de5.jpg</td>\n",
       "      <td>264661 17 265429 33 266197 33 266965 33 267733...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>360486 1 361252 4 362019 5 362785 8 363552 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>51834 9 52602 9 53370 9 54138 9 54906 9 55674 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId                                      EncodedPixels\n",
       "0  00003e153.jpg                                                NaN\n",
       "1  0001124c7.jpg                                                NaN\n",
       "2  000155de5.jpg  264661 17 265429 33 266197 33 266965 33 267733...\n",
       "3  000194a2d.jpg  360486 1 361252 4 362019 5 362785 8 363552 10 ...\n",
       "4  000194a2d.jpg  51834 9 52602 9 53370 9 54138 9 54906 9 55674 ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ship_dir = '../data/'\n",
    "train_image_dir = os.path.join(ship_dir, 'train_v2')\n",
    "test_image_dir = os.path.join(ship_dir, 'test_v2')\n",
    "\n",
    "masks = pd.read_csv(os.path.join(ship_dir, 'train_ship_segmentations_v2.csv'))\n",
    "print(masks.shape[0], 'masks found')\n",
    "print(masks['ImageId'].value_counts().shape[0], 'unique images found')\n",
    "masks.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add samples images from the dataset (at least 1 with a ship)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using code from a getting started notebook on the kaggle project, we are able to see the masks. below we are looking at an image with the max number of ship (15). This code checks the given encoding and decoding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Decoding->Encoding RLE_0: 15 -> RLE_1: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from util import masks_as_image\n",
    "from util import multi_rle_encode\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "rle_0 = masks.query('ImageId==\"11f3bae66.jpg\"')['EncodedPixels'] # 15 ships in the image\n",
    "img_0 = masks_as_image(rle_0)\n",
    "ax1.imshow(img_0[:, :, 0])\n",
    "ax1.set_title('Image$_0$')\n",
    "rle_1 = multi_rle_encode(img_0)\n",
    "img_1 = masks_as_image(rle_1)\n",
    "ax2.imshow(img_1[:, :, 0])\n",
    "ax2.set_title('Image$_1$')\n",
    "print('Check Decoding->Encoding',\n",
    "      'RLE_0:', len(rle_0), '->',\n",
    "      'RLE_1:', len(rle_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier\n",
    "\n",
    "The objective of our binary classifier is to gain a understanding of building models in Tensorflow and apply optimation problems learned in this class to TensorFlow. Out Binary classifier will use the data from the Airbus challenge and instead of use image segmentation to predict where a ship is in a image, it will simply detect if there is a ship in the image. For this, we will use a CNN model and explore training and optimzation in Tensorflow.\n",
    "\n",
    "### Adjust the data for image classification.\n",
    "\n",
    "The first step to update the dataframe for the classification. To start we have pixel encoding and we need to translate the to classifications of `ship` and `no_ship`. When a image has mutliple ships there are multiple entries in the dataframe, so they will need to be flattened aswell. Below is the code we used to to that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>ship_count</th>\n",
       "      <th>has_ship</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192265</th>\n",
       "      <td>ff98308ec.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182939</th>\n",
       "      <td>f34683d2a.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132205</th>\n",
       "      <td>afbbeef01.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49326</th>\n",
       "      <td>41b58d393.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16104</th>\n",
       "      <td>156cd60c2.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_ship</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ImageId  ship_count  has_ship    class\n",
       "192265  ff98308ec.jpg           0         0  no_ship\n",
       "182939  f34683d2a.jpg           0         0  no_ship\n",
       "132205  afbbeef01.jpg           3         1     ship\n",
       "49326   41b58d393.jpg           0         0  no_ship\n",
       "16104   156cd60c2.jpg           0         0  no_ship"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# populate a ship count (0 or 1)\n",
    "masks['ship_count'] = masks['EncodedPixels'].map(lambda x: 1 if isinstance(x, str) else 0)\n",
    "masks.sample(10)\n",
    "\n",
    "# aggregate ship images and sum number of ships. images with multiple ships will have multiple rows\n",
    "unique_masks = masks.groupby('ImageId').agg({'ship_count': 'sum'}).reset_index()\n",
    "unique_masks['file_size_kb'] = unique_masks['ImageId'].map(lambda c_img_id: os.stat(os.path.join(train_image_dir, c_img_id)).st_size/1024 if os.path.exists(os.path.join(train_image_dir, c_img_id)) else 0.0)\n",
    "unique_masks = unique_masks[unique_masks['file_size_kb']>50]\n",
    "unique_masks.drop(['file_size_kb'], axis=1, inplace=True)\n",
    "\n",
    "# create a string class column, used later for the build in image generator\n",
    "unique_masks['has_ship'] = unique_masks['ship_count'].map(lambda x: 1 if x > 0.0 else 0)\n",
    "unique_masks['class'] = unique_masks['ship_count'].map(lambda x: 'ship' if x > 0.0 else 'no_ship')\n",
    "unique_masks.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification Training & Results\n",
    "\n",
    "### Baseline CNN model\n",
    "\n",
    "To start and to get familiar with TensorFlow, we started with a very simple CNN model build in TensorFlow2 (Keras). This model includes 3 hidden convolution layer with 16, 32 and 64 nodes respectively. Each convoluational layer has a max pooling layer with the deful `(2, 2)` pool sizing. Our image generator reduced the size of the ships from `(768, 768, 3)` to `(256, 256, 3`) to reduce training times. The model uses the standard Adam optimzer with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(256, 256 ,3)),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this model gives us a baseline for optimzation. \n",
    "\n",
    "<img src=\"images/binary_classifier_results3.png\" alt=\"initial binary classifier\" style=\"float: left; height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't perform horribly, but we can clearly see some issues with overfitting in the training set and the validation accurarcy doesn't improve much after the first few epochs. In the next few iterations we will try and improve this by adding some common techiques to reduce overflow and produce better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterations from the baseline\n",
    "\n",
    "This is a consolidation to reduce the size of our report. We tired a few iterations of the following:\n",
    "\n",
    "* **Undersamplying the data** -- reduce the training data to have a more even distrubution of the binary classes.\n",
    "* **Augmenting the data** -- Randomly flipping and slightly roatating training images to avoid overfitting\n",
    "* **Momentum/Adom optimization** -- Adding beta values to the Adam optimzier\n",
    "* **Learning Rate Reduction on Plateau**: When the accurarcy of loss of validation flatlines, reduce the learning rate by a factor\n",
    "* **Drop out/Larget Model** -- Randomly dropping nodes in the model, this again can avoid overfitting and in some cases improve the model\n",
    "\n",
    "#### 1 . Undersampling the data\n",
    "The first thing we can do is undersample the training data. We do this because the distrubution is greatly inbalanced. Threre are many more images with no ships and therefore our model will be bias to guess no ships. Predicting no ships gives the model about 80% accurarcy given the distrbution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_ship count: 149293\n",
      "ship count:     41996\n",
      "\n",
      "balanced no ship count: 46996\n"
     ]
    }
   ],
   "source": [
    "no_ship_count = unique_masks[unique_masks['class'] == 'no_ship'].shape[0]\n",
    "ship_count = unique_masks[unique_masks[\"class\"] == 'ship'].shape[0]\n",
    "print('no_ship count: {}\\nship count:     {}\\n'.format(no_ship_count, ship_count))\n",
    "no_ship_idxs = unique_masks[unique_masks['class'] == 'no_ship'].sample(no_ship_count - ship_count - 5000).index\n",
    "balanced_train_df = unique_masks.drop(no_ship_idxs, inplace=False)\n",
    "no_ship_count = balanced_train_df[balanced_train_df['class'] == 'no_ship'].shape[0]\n",
    "print('balanced no ship count: {}'.format(no_ship_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Augmenting the data\n",
    "Using the built in image generators provided by Tensorflow2 we can easily augment the data to avoid overfitting. We did this by adding horizontal flip and roatation to our images. We did not want to add any transformations such as stretching because of the nature of our data. The boats are very small and generally the same shape, so altering the structure of the image, we believe, would be detramental. The following in the image generator we used to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_image_generator = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10, # rotate +/- 10 degrees\n",
    "    horizontal_flip=True # flip the image\n",
    ")\n",
    "\n",
    "train_data_gen = train_image_generator.flow_from_dataframe(\n",
    "    balanced_train_df,\n",
    "    directory=train_image_dir,\n",
    "    x_col='ImageId',\n",
    "    y_col='class',\n",
    "    target_size=(256, 256),\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adam Optimization\n",
    "We also played around with the Adam optimizer adding beta values. We did not see any drastic changes in the first few epochs so we kept the default values for beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=10e-4, \n",
    "    beta_1=0.9, \n",
    "    beta_2=0.999, \n",
    "    epsilon=1e-07,\n",
    "    name='Adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Learning Rate Reduction on Plateau\n",
    "TensorFlow provides a number of built in callbacks that can be used to help train your model. We used a few to experienment including TensorBoard, Early Stopping, Reduce Learning Rate on Plateu, and Checkpoints. Early stopping is extremely helpful when training on instances that cost money (assuming you know when the model stops). Early stoping will measure a metric of the model, usually a validation metric, and stop the model if that metric doesn't improve for some number of epochs. Reduce Learning Rate on Plateau is also very useful. Instead of adjust the learning rate linearly or based on a aribritrary epoch threashold, this callback will start to reduce the learning rate when the model stops learning. These callbacks are calculated after each epoch and action are taking on training if needed. Engineers can also create custom callbacks to monitor or report on the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceLROnPlateautf = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', \n",
    "    factor=0.1, \n",
    "    patience=3, \n",
    "    verbose=1, \n",
    "    mode='auto',\n",
    "    min_delta=0, \n",
    "    cooldown=0, \n",
    "    min_lr=0\n",
    ")\n",
    "\n",
    "earlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    min_delta=0, \n",
    "    patience=10, \n",
    "    verbose=1, \n",
    "    mode='max',\n",
    "    baseline=None, \n",
    "    restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dropout & Larger Model\n",
    "Applying dropout to the model with the keras `Dropout()` keras layer helper is a way to reduce overfitting the training data. Dropout is a techique that randomly inhibits nodes in the network \n",
    "\n",
    "Below is the final model we attamped, which is slightly larger than the baseline (4 convolution layers with 32, 32, 64, 64 nodes), and adds dropout at 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(256, 256 ,3)),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of optimization\n",
    "We were able to reduce the about of overfitting with our data, but we were never able to get the model to learn past about the 92% validation accurarcy. We trained about 10 iterations of the model, each taking about 70 - 90 minutes to complete. With this we determine that the data may not be as clean has we hoped. In a few cases we did see  images with mis numbered boats, or that were clearly generated (repeated mirror images, etc).\n",
    "\n",
    "Below is the learning curve from our more complex model with dropout also utilizing the learning rate reduction callback and data augmentation. The overfitting has clearly improved from the baseline, but there is no \n",
    "\n",
    "<img src=\"images/dropout_binary.png\" alt=\"initial binary classifier\" style=\"float: left; height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net Model\n",
    "\n",
    "Building upon our binary classification experiments, we wanted to implement a deep learning architecture that would actually be able to not just identify whether an image had a ship or not, but to also identify the number of ships in the image. To accomplish this, we chose to implement a U-Net architecture. \n",
    "\n",
    "The U-Net was first introduced in 2015 as a convolutional neural network developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg, Germany. The full paper for the original U-Net can be found [here.](https://arxiv.org/abs/1505.04597) \n",
    "\n",
    "The reason behind choosing the U-Net architecture is partly because the U-Net is a very popular end-to-end encoder-decoder network for image segmentation. In our implementation, the U-Net detects individual objects (ships) and predicts their masks.\n",
    "\n",
    "\n",
    "### U-Net Architecture\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/921/0*o1n7BqtOj9_xLD_x.png\" alt=\"initial binary classifier\" style=\"float: left; height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The U-Net model is shaped like a \"U\", hence its name. The model architecture is symmetric and consists of two major parts - a contracting path and an expansive path. Our implementation is given in the code chunk below, which utilizes the Keras Tensorflow library. The imported functions can be found in our utility file.\n",
    "\n",
    "```python\n",
    "from util import conv_down #These auxiliary functions are in our util file\n",
    "from util import conv_up\n",
    "from util import pool\n",
    "\n",
    "d1 = conv_down(8, input_img, name='d1')\n",
    "dp1 = pool(d1, name='d1')\n",
    "d2 = conv_down(16, dp1, name='d2')\n",
    "dp2 = pool(d2, name='d2')\n",
    "d3 = conv_down(32, dp2, name='d3')\n",
    "dp3 = pool(d3, name='d3')\n",
    "d4 = conv_down(64, dp3, name='d4')\n",
    "dp4 = pool(d4, name='d4')\n",
    "b = conv_down(128, dp4, name='b')\n",
    "u1 = conv_up(64, b, d4, name='u1')\n",
    "u2 = conv_up(32, u1, d3, name='u2')\n",
    "u3 = conv_up(16, u2, d2, name='u3')\n",
    "u4 = conv_up(8, u3, d1, name='u4')\n",
    "\n",
    "out = Conv2D(1, (1, 1), activation='sigmoid', name='out_conv1')(u4)\n",
    "out = Cropping2D((unet_params.edge_crop, unet_params.edge_crop), name='out_crop')(out)\n",
    "out = ZeroPadding2D((unet_params.edge_crop, unet_params.edge_crop), name='out_pad')(out)\n",
    "\n",
    "unet = models.Model(inputs=[input_img], outputs=[out], name=\"UNet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contraction\n",
    "\n",
    "During the contraction phase, each layer typically undergoes the same path - two convolution layers, followed by a max pooling layer, and a dropout layer. In our implementation, we don't use a dropout layer.\n",
    "\n",
    "We first start on the left side of the \"U\", where we contract the input images. The first part of our implementation is:\n",
    "\n",
    "```python\n",
    "d1 = conv_down(8, input_img, name='d1')\n",
    "dp1 = pool(d1, name='d1')\n",
    "```\n",
    "Where conv down and pool are:\n",
    "\n",
    "\n",
    "```python\n",
    "def conv_down(filter_, in_layer, name, kernel=(3, 3), activation='relu', padding='same'):\n",
    "    l = Conv2D(filter_, kernel, activation=activation, padding=padding, name=name+'_conv1')(in_layer)\n",
    "    l = Conv2D(filter_, kernel, activation=activation, padding=padding, name=name+'_conv2')(l)\n",
    "    return l\n",
    "\n",
    "def pool(in_layer, name, pool_size=(2, 2)):\n",
    "    return MaxPooling2D(pool_size, name=name+'_pool')(in_layer)\n",
    "```\n",
    "\n",
    "\n",
    "Here, the \"d1\" variable consists of two convolutional layers (since we are doing convolutions on a 3x3 basis), which serve to increase the depth of the input image. This is followed by a max pooling process which essentially cuts the size of the image in half. In our implementation, the number of channels start at 8, and increase to 16, while the pixel size of the image goes from 586x568 to 284x284.\n",
    "\n",
    "This process is then repeated three more times\n",
    "\n",
    "```python\n",
    "d2 = conv_down(16, dp1, name='d2')\n",
    "dp2 = pool(d2, name='d2')\n",
    "d3 = conv_down(32, dp2, name='d3')\n",
    "dp3 = pool(d3, name='d3')\n",
    "d4 = conv_down(64, dp3, name='d4')\n",
    "dp4 = pool(d4, name='d4')\n",
    "```\n",
    "until the input image reaches the middle/lowest layer. The image is then run through two more convolutional layers, but no max pooling layer, since the next step is expansion. At this point, the image has been resized to 28x28x128, and is ready to get upsized to its original size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expansion\n",
    "\n",
    "The flow of an expansion layer consists of upsampling the input image and concatenating it with the corresponding image from the contracting path. In our implementation, it looks like this:\n",
    "\n",
    "\n",
    "```python\n",
    "u1 = conv_up(64, b, d4, name='u1')\n",
    "u2 = conv_up(32, u1, d3, name='u2')\n",
    "u3 = conv_up(16, u2, d2, name='u3')\n",
    "u4 = conv_up(8, u3, d1, name='u4')\n",
    "```\n",
    "\n",
    "Where conv_up is:\n",
    "\n",
    "\n",
    "```python\n",
    "def conv_up(filter_, in_layer, conv_down_layer, name, upsample_size=(2, 2), kernel=(3, 3), activation='relu', padding='same'):\n",
    "    l = UpSampling2D(upsample_size, name=name+'_upsample')(in_layer)\n",
    "    l = concatenate([l, conv_down_layer], name=name+'_concat')\n",
    "    l = Conv2D(filter_, kernel, activation=activation, padding=padding, name=name+'_conv1')(l)\n",
    "    l = Conv2D(filter_, kernel, activation=activation, padding=padding, name=name+'_conv2')(l)\n",
    "    return l\n",
    "```\n",
    "\n",
    "Here, we apply the UpSampling2D function from Keras, which essentially performs some padding on the original image, followed by a convolution operation (transposed convolution). After upsampling, the image size is doubled, and the image is then concatenated with the corresponding image from the corresponding contraction layer. That is, in the first layer the image is upsized from 28x28x128 to 56x56x64, and concatenated with the 56x56x64 image in the contraction layer (u1 and d4 in the first case) to make an image of size 56x56x128. By concatenating the images, information from the previous layers is combined to obtain a more precise prediciton. This process is again repeated 3 times to preserve symmetry, and trivially, an unsymmetric model would not be able to perform concatenation in the expansion phase.\n",
    "\n",
    "## Output Layer\n",
    "\n",
    "Once the image has gotten to the top layer in the expansion phase, the final step in the model requires us to reshape the image to satisfy the prediction requirements.\n",
    "\n",
    "```python\n",
    "out = Conv2D(1, (1, 1), activation='sigmoid', name='out_conv1')(u4)\n",
    "out = Cropping2D((unet_params.edge_crop, unet_params.edge_crop), name='out_crop')(out)\n",
    "out = ZeroPadding2D((unet_params.edge_crop, unet_params.edge_crop), name='out_pad')(out)\n",
    "```\n",
    "\n",
    "\n",
    "The output layer is a convolution layer with a single filter of size 1x1. Here, we add additional padding to fit the pixel size requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra\n",
    "\n",
    "\n",
    "Based on the kaggle challenge [Airbus Ship Detection Challenge](https://www.kaggle.com/c/airbus-ship-detection/overview). This challenge uses satellite images to detect if a image has ships and draw a box (mask) around the ship. This challenge contains ~40GB of data and the training size is a little out of scope for this project, so we approaching it with two goals:\n",
    "1. Training a binary classifier to classify images that contain ship(s)\n",
    "2. Explore the state-of-the-art approach to this model, U-Net\n",
    "    - Goal is to follow the baseline notebook submission and understand the architechure. \n",
    "    \n",
    "    \n",
    "The objective of a binary classifier model is predicting on the definition of binary classification which is the task of classifying a set of data into two distinct groups. The entire premise of creating a binary classification neural network model is to end up with an output layer which has 2 classes (1 or 0). In our case, 1 signifying that an image contains a ship while 0 indicating that the image does not contain an image of a ship. Much like any other neural network, this model requires an input layer, hidden layers, and an output layer. Our images had to be flattened to create a vector rather than a matrix for each image which was fed into the input layer of our model. After being multiplied by a bias and weights, the resulting vectors are run through an activation function which then propagates forwards. This model also utilizes cross entropy as its cost function as it is well suited to support a classification problem such as ours.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
